{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smishing detection using [danielhenel/smishing-detection-llama-3-8B-instruct](https://huggingface.co/danielhenel/smishing-detection-llama-3-8B-instruct) - evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniel\\Desktop\\repo\\research\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "import pickle\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 954 hams and 161 smishes in the test dataset.\n"
     ]
    }
   ],
   "source": [
    "# test data\n",
    "with open(\"./data/test_data.pkl\", \"rb\") as input_file:\n",
    "    test_data = pickle.load(input_file)\n",
    "\n",
    "X_test = test_data[\"X_test\"]\n",
    "y_test = test_data[\"y_test\"]\n",
    "\n",
    "total_hams_count = 0\n",
    "total_smishes_count = 0\n",
    "\n",
    "for label in y_test:\n",
    "    if label == \"ham\":\n",
    "        total_hams_count += 1\n",
    "    if label == \"smish\":\n",
    "        total_smishes_count += 1\n",
    "\n",
    "print(\"There is {} hams and {} smishes in the test dataset.\".format(total_hams_count, total_smishes_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.54s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Model name\n",
    "base_model = \"NousResearch/Meta-Llama-3-8B-Instruct\"\n",
    "adapter_model = 'danielhenel/smishing-detection-llama-3-8B-instruct'\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "attn_implementation = \"eager\"\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0},\n",
    "    attn_implementation=attn_implementation\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "model.load_adapter(adapter_model)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_model, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "false_hams_indicies = []\n",
    "false_smishes_indicies = []\n",
    "false_hams_count = 0\n",
    "false_smishes_count = 0\n",
    "true_hams_count = 0\n",
    "true_smishes_count = 0\n",
    "errors_count = 0\n",
    "errors_indicies = []\n",
    "errors = []\n",
    "\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    prompt = (\n",
    "    \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\"\n",
    "    \"Do you think it is a ham or smish message? \"\n",
    "    \"Your output should be a single word 'smish' or 'ham'. \"\n",
    "    \"Do not write a sentence. \"\n",
    "    \"Output is case-sensitive. \"\n",
    "    \"SMS content: {}\"\n",
    "    \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "    ).format(X_test[i])\n",
    "\n",
    "    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500)\n",
    "    answer = pipe(prompt)\n",
    "    answer = answer[0]['generated_text'].split(\"<|start_header_id|>assistant<|end_header_id|>\")[1].lower().strip()\n",
    "\n",
    "    if answer not in [\"ham\", \"smish\"]:\n",
    "        errors_count += 1\n",
    "        errors_indicies.append(i)\n",
    "        errors.append(answer)\n",
    "        continue\n",
    "    elif answer == \"ham\" and y_test[i] == \"ham\": # correctly recognized as a ham\n",
    "        true_hams_count += 1\n",
    "    elif answer == \"smish\" and y_test[i] == \"smish\": # correctly recognized as a smish\n",
    "        true_smishes_count += 1\n",
    "    elif answer == \"ham\" and y_test[i] == \"smish\": # wrongly recognized as a ham\n",
    "        false_hams_indicies.append(i)\n",
    "        false_hams_count += 1\n",
    "    elif answer == \"smish\" and y_test[i] == \"ham\": # wrongly recognized as a smish\n",
    "        false_smishes_indicies.append(i)\n",
    "        false_smishes_count += 1\n",
    "        \n",
    "# errors warning   \n",
    "if errors_count != 0:\n",
    "    if errors_count == 1:\n",
    "        print(\"WARNING: {} error\".format(errors_count))\n",
    "    else:\n",
    "        print(\"WARNING: {} errors\".format(errors_count))\n",
    "\n",
    "# save results for further analysis\n",
    "results = {\"FN\" : false_hams_count, \"FP\" : false_smishes_count, \n",
    "           \"TN\" : true_hams_count, \"TP\" : true_smishes_count,\n",
    "           \"FN_indicies\" : false_hams_indicies, \"FP_indicies\" : false_smishes_indicies,\n",
    "            \"errors_count\" : errors_count, \"errors\" : errors, \"errors_indicies\" : errors_indicies}\n",
    "\n",
    "with open(\"./results/results_llama_3_8b_fine_tuned.pkl\", 'wb') as handle:\n",
    "    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The performance of our fine-tuned Llama 3 8b in smishing detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 161, TN: 927, FP: 27, FN: 0, ERRORS: 0\n",
      "accuracy of acceptable answers only: 97.58%, general accuracy: 97.58%\n",
      "F1 score of acceptable answers only: 92.26%, general F1 score: 92.26%\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"./results/results_llama_3_8b_fine_tuned.pkl\", \"rb\") as input_file:\n",
    "    results = pickle.load(input_file)\n",
    "\n",
    "FN = results['FN']  #FN - messages wrongly recognized as not smishes (hams)\n",
    "FP = results['FP']  #FP - messages wrongly recognized as smishes\n",
    "TN = results['TN']  #TN - messages correctly recognized as not smishes (hams)\n",
    "TP = results['TP']  #TP - messages correctly recognized as smishes\n",
    "ERRORS = results['errors_count']  #ERRORS - non-acceptable answers\n",
    "TOTAL = FN + FP + TN + TP\n",
    "    \n",
    "TP = results['TP']\n",
    "TN = results['TN']\n",
    "FP = results['FP']\n",
    "FN = results['FN']\n",
    "ERRORS = results['errors_count']\n",
    "TOTAL = FN + FP + TN + TP\n",
    "\n",
    "print(\"TP: {}, TN: {}, FP: {}, FN: {}, ERRORS: {}\".format(TP,TN,FP,FN,ERRORS))\n",
    "\n",
    "accuracy_acc = ((TP + TN) / TOTAL)\n",
    "accuracy_gen = ((TP + TN) / TOTAL) * (TOTAL / (TOTAL + ERRORS))\n",
    "print(\"accuracy of acceptable answers only: {0:.2f}%, general accuracy: {1:.2f}%\".format(accuracy_acc * 100, accuracy_gen * 100))\n",
    "\n",
    "F1_score_acc = TP / (TP + (FP + FN) / 2)\n",
    "F1_score_gen = TP / (TP + (FP + FN) / 2) * (TOTAL / (TOTAL + ERRORS))\n",
    "print(\"F1 score of acceptable answers only: {0:.2f}%, general F1 score: {1:.2f}%\".format(F1_score_acc * 100, F1_score_gen * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
