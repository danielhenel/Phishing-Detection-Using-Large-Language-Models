{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smishing detection using [danielhenel/smishing-detection-llama-3-8B-instruct](https://huggingface.co/danielhenel/smishing-detection-llama-3-8B-instruct) - evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniel\\Desktop\\repo\\research\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "import pickle\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 954 hams and 161 smishes in the test dataset.\n"
     ]
    }
   ],
   "source": [
    "# test data\n",
    "with open(\"./data/test_data.pkl\", \"rb\") as input_file:\n",
    "    test_data = pickle.load(input_file)\n",
    "\n",
    "X_test = test_data[\"X_test\"]\n",
    "y_test = test_data[\"y_test\"]\n",
    "\n",
    "total_hams_count = 0\n",
    "total_smishes_count = 0\n",
    "\n",
    "for label in y_test:\n",
    "    if label == \"ham\":\n",
    "        total_hams_count += 1\n",
    "    if label == \"smish\":\n",
    "        total_smishes_count += 1\n",
    "\n",
    "print(\"There is {} hams and {} smishes in the test dataset.\".format(total_hams_count, total_smishes_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.54s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Model name\n",
    "base_model = \"NousResearch/Meta-Llama-3-8B-Instruct\"\n",
    "adapter_model = 'danielhenel/smishing-detection-llama-3-8B-instruct'\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "attn_implementation = \"eager\"\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0},\n",
    "    attn_implementation=attn_implementation\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "model.load_adapter(adapter_model)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_model, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "false_hams_indicies = []\n",
    "false_smishes_indicies = []\n",
    "false_hams_count = 0\n",
    "false_smishes_count = 0\n",
    "true_hams_count = 0\n",
    "true_smishes_count = 0\n",
    "errors_count = 0\n",
    "errors_indicies = []\n",
    "errors = []\n",
    "\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    prompt = (\n",
    "    \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\"\n",
    "    \"Do you think it is a ham or smish message? \"\n",
    "    \"Your output should be a single word 'smish' or 'ham'. \"\n",
    "    \"Do not write a sentence. \"\n",
    "    \"Output is case-sensitive. \"\n",
    "    \"SMS content: {}\"\n",
    "    \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "    ).format(X_test[i])\n",
    "\n",
    "    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500)\n",
    "    answer = pipe(prompt)\n",
    "    answer = answer[0]['generated_text'].split(\"<|start_header_id|>assistant<|end_header_id|>\")[1].lower().strip()\n",
    "\n",
    "    if answer not in [\"ham\", \"smish\"]:\n",
    "        errors_count += 1\n",
    "        errors_indicies.append(i)\n",
    "        errors.append(answer)\n",
    "        continue\n",
    "    elif answer == \"ham\" and y_test[i] == \"ham\": # correctly recognized as a ham\n",
    "        true_hams_count += 1\n",
    "    elif answer == \"smish\" and y_test[i] == \"smish\": # correctly recognized as a smish\n",
    "        true_smishes_count += 1\n",
    "    elif answer == \"ham\" and y_test[i] == \"smish\": # wrongly recognized as a ham\n",
    "        false_hams_indicies.append(i)\n",
    "        false_hams_count += 1\n",
    "    elif answer == \"smish\" and y_test[i] == \"ham\": # wrongly recognized as a smish\n",
    "        false_smishes_indicies.append(i)\n",
    "        false_smishes_count += 1\n",
    "        \n",
    "# errors warning   \n",
    "if errors_count != 0:\n",
    "    if errors_count == 1:\n",
    "        print(\"WARNING: {} error\".format(errors_count))\n",
    "    else:\n",
    "        print(\"WARNING: {} errors\".format(errors_count))\n",
    "\n",
    "# save results for further analysis\n",
    "results = {\"FN\" : false_hams_count, \"FP\" : false_smishes_count, \n",
    "           \"TN\" : true_hams_count, \"TP\" : true_smishes_count,\n",
    "           \"FN_indicies\" : false_hams_indicies, \"FP_indicies\" : false_smishes_indicies,\n",
    "            \"errors_count\" : errors_count, \"errors\" : errors, \"errors_indicies\" : errors_indicies}\n",
    "\n",
    "with open(\"./results/results_llama_3_8b_fine_tuned.pkl\", 'wb') as handle:\n",
    "    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The performance of our fine-tuned Llama 3 8b in smishing detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FN = false_hams_count     #FN - messages wrongly recognized as not smishes (hams)\n",
    "FP = false_smishes_count  #FP - messages wrongly recognized as smishes\n",
    "TN = true_hams_count      #TN - messages correctly recognized as not smishes (hams)\n",
    "TP = true_smishes_count   #TP - messages correctly recognized as smishes\n",
    "TOTAL = FN + FP + TN + TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages wrongly recognized as hams: 0.00%\n",
      "Messages wrongly recognized as smishes: 2.42%\n",
      "Messages correctly recognized as hams: 83.14%\n",
      "Messages correctly recognized as smishes: 14.44%\n"
     ]
    }
   ],
   "source": [
    "print(\"Messages wrongly recognized as hams: {0:.2f}%\".format(FN / TOTAL * 100))\n",
    "print(\"Messages wrongly recognized as smishes: {0:.2f}%\".format(FP / TOTAL * 100))\n",
    "print(\"Messages correctly recognized as hams: {0:.2f}%\".format(TN / TOTAL * 100))\n",
    "print(\"Messages correctly recognized as smishes: {0:.2f}%\".format(TP / TOTAL * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.58%\n"
     ]
    }
   ],
   "source": [
    "accuracy = (TP + TN) / TOTAL\n",
    "print(\"{0:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00%\n"
     ]
    }
   ],
   "source": [
    "recall = TP / (TP + FN)\n",
    "print(\"{0:.2f}%\".format(recall * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.64%\n"
     ]
    }
   ],
   "source": [
    "precision = TP / (TP + FP)\n",
    "print(\"{0:.2f}%\".format(precision * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.26%\n"
     ]
    }
   ],
   "source": [
    "F1_score = TP / (TP + (FP + FN) / 2)\n",
    "print(\"{0:.2f}%\".format(F1_score * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
