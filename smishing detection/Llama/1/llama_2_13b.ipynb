{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smishing detection using [NousResearch/Llama-2-13b-chat-hf](https://huggingface.co/NousResearch/Llama-2-13b-chat-hf) - evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM, pipeline\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "with open(\"./../data/test_data.pkl\", \"rb\") as input_file:\n",
    "    test_data = pickle.load(input_file)\n",
    "\n",
    "X_test = test_data[\"X_test\"]\n",
    "y_test = test_data[\"y_test\"]\n",
    "\n",
    "total_hams_count = 0\n",
    "total_smishes_count = 0\n",
    "\n",
    "for label in y_test:\n",
    "    if label == \"ham\":\n",
    "        total_hams_count += 1\n",
    "    if label == \"smish\":\n",
    "        total_smishes_count += 1\n",
    "\n",
    "print(\"There is {} hams and {} smishes in the test dataset.\".format(total_hams_count, total_smishes_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model name\n",
    "model_name = 'NousResearch/Llama-2-13b-chat-hf'\n",
    "\n",
    "# Load the model\n",
    "model = LlamaForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_hams_indicies = []\n",
    "false_smishes_indicies = []\n",
    "false_hams_count = 0\n",
    "false_smishes_count = 0\n",
    "true_hams_count = 0\n",
    "true_smishes_count = 0\n",
    "errors_count = 0\n",
    "errors_indicies = []\n",
    "errors = []\n",
    "\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    prompt = (\n",
    "    \"<s>[INST] Do you think it is a ham or smish message? \"\n",
    "    \"Your output should be a single word 'smish' or 'ham'. \"\n",
    "    \"Do not write a sentence. \"\n",
    "    \"Output is case-sensitive. \"\n",
    "    \"SMS content: {}[/INST]\"\n",
    "    ).format(X_test[i])\n",
    "\n",
    "    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "    answer = pipe(prompt)\n",
    "\n",
    "    if answer not in [\"ham\", \"smish\"]:\n",
    "        errors_count += 1\n",
    "        errors_indicies.append(i)\n",
    "        errors.append(answer)\n",
    "        continue\n",
    "    elif answer == \"ham\" and y_test[i] == \"ham\": # correctly recognized as a ham\n",
    "        true_hams_count += 1\n",
    "    elif answer == \"smish\" and y_test[i] == \"smish\": # correctly recognized as a smish\n",
    "        true_smishes_count += 1\n",
    "    elif answer == \"ham\" and y_test[i] == \"smish\": # wrongly recognized as a ham\n",
    "        false_hams_indicies.append(i)\n",
    "        false_hams_count += 1\n",
    "    elif answer == \"smish\" and y_test[i] == \"ham\": # wrongly recognized as a smish\n",
    "        false_smishes_indicies.append(i)\n",
    "        false_smishes_count += 1\n",
    "        \n",
    "# errors warning   \n",
    "if errors_count != 0:\n",
    "    if errors_count == 1:\n",
    "        print(\"WARNING: {} error\".format(errors_count))\n",
    "    else:\n",
    "        print(\"WARNING: {} errors\".format(errors_count))\n",
    "\n",
    "# save results for further analysis\n",
    "results = {\"FN\" : false_hams_count, \"FP\" : false_smishes_count, \n",
    "           \"TN\" : true_hams_count, \"TP\" : true_smishes_count,\n",
    "           \"FN_indicies\" : false_hams_indicies, \"FP_indicies\" : false_smishes_indicies,\n",
    "            \"errors_count\" : errors_count, \"errors\" : errors, \"errors_indicies\" : errors_indicies}\n",
    "\n",
    "with open(\"./results/results_llama_2_13b.pkl\", 'wb') as handle:\n",
    "    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The performance of our fine-tuned Llama 2 7b in smishing detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FN = false_hams_count     #FN - messages wrongly recognized as not smishes (hams)\n",
    "FP = false_smishes_count  #FP - messages wrongly recognized as smishes\n",
    "TN = true_hams_count      #TN - messages correctly recognized as not smishes (hams)\n",
    "TP = true_smishes_count   #TP - messages correctly recognized as smishes\n",
    "TOTAL = FN + FP + TN + TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages wrongly recognized as hams: {0:.2f}%\".format(FN / TOTAL * 100))\n",
    "print(\"Messages wrongly recognized as smishes: {0:.2f}%\".format(FP / TOTAL * 100))\n",
    "print(\"Messages correctly recognized as hams: {0:.2f}%\".format(TN / TOTAL * 100))\n",
    "print(\"Messages correctly recognized as smishes: {0:.2f}%\".format(TP / TOTAL * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (TP + TN) / TOTAL\n",
    "print(\"{0:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = TP / (TP + FN)\n",
    "print(\"{0:.2f}%\".format(recall * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = TP / (TP + FP)\n",
    "print(\"{0:.2f}%\".format(precision * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_score = TP / (TP + (FP + FN) / 2)\n",
    "print(\"{0:.2f}%\".format(F1_score * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
